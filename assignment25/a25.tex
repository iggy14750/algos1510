\documentclass{article}
\author{Isaac B Goss\\ James Hahn\\ Jonathan Dyer}
\title{Assignment 25}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage[margin=0.8in]{geometry}
\usepackage{graphicx}

% ============ USED FOR OUR FORMAT ============
\newtheorem{thm}{Claim}
\newtheorem{lemma}{Lemma}
\providecommand{\prob}[1]{\section*{Problem #1}}
\providecommand{\soln}{\textbf{Solution: }}
\providecommand{\image}[1]{
    \begin{center}
        \includegraphics%[width=0.95\textwidth]
            {#1}
    \end{center}
}
\providecommand{\tightlist}{
    \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}
}
\providecommand{\reducible}[2]{
  \textbf{#1} $\leq$ \textbf{#2}
}

% ============ USED FOR CODE LISTINGS ============
\usepackage{listings}
\usepackage[usenames,dvipsnames,svgnames]{xcolor}
\definecolor{javagreen}{rgb}{0.25,0.5,0.35}
\lstset{
    basicstyle   = \footnotesize,
    commentstyle = \color{javagreen},
    frame        = single,
    language     = C,
    stringstyle  = \color{orange},
    numbers      = left,
    showstringspaces=false,
    deletekeywords = {len, max, format, min},
    morekeywords = {yield, function, then, do, to},
    keywordstyle = \color{blue},
    escapeinside={(*}{*)},
    mathescape
}


\begin{document}
\maketitle

\prob{23}
We can say that \reducible{Vertex Cover}{River Crossing} by the following reduction:
\begin{lstlisting}
function VertexCover(G, k)
    return RiverCrossing(G, k)
\end{lstlisting}
\begin{proof}
 We must establish that a $k$-cover is possible iff a $k$-crossing is possible.
 \begin{itemize}
  \item $k$-cover impossible $\implies$ $k$-crossing impossible.
        If this is the case, then we cannot make the first move.
        Attempting to take any subset of vertices without breaking up every edge means we have lost the game.

  \item $k$-cover possible $\implies$ $k$-crossing possible.
        \begin{lemma}
         If a graph $G$ has a $k$-cover, then all subgraphs of $G$ contain a $k$-cover.
        \end{lemma}

        This tells us that it is always possible to collect a subset of vertices so that either side of the river is edge-free.

        Thus, the only issue is whether we can complete the entire move.
        I am going to say here that large cliques are the hardest problem.
        That is, in the sense that all subproblems possible on some number of vertices are subproblems of the clique on those vertices.
        I want to show that even in these hardest of problems, having a $k$-cover available implies it is possible to find a $k$-crossing.

        So, let's say we have a $k+1$-clique to start out with.
        Well, then we move a $k$-clique over, leaving one lone vertex.
        But we can't leave this entire clique and go back, so we leave just one vertex, and carry our $k-1$-cover back.
        We then join back up with our vertex left on the left, invite him to our boat, and complete the challenge.

        This can be extended to support graphs which contain a $k+1$-clique as a proper subgraph but have a $k$-cover, by moving vertices outside of the clique one at a time, with our $k-1$ cover on board.

 \end{itemize}
\end{proof}


\prob{5}
On an EREW PRAM with $n$ processors, we get the following algorithm for computing $n!$ in $O(lg(n))$ time:\\
\begin{lstlisting}
fact($k, n, p$)   // k is the starting value for our partial product
    if k == n then return 1
    else return fact($k, \frac{n+k}{2}, \frac{p}{2}$) $\times$ fact($\frac{n+k}{2}, n, \frac{p}{2}$)
\end{lstlisting}
Obviously the first call would be $fact(1, n, n)$ for computing $n!$ with $n$ processors. By the same recursive call tree and calculations we did in class for performing sums in parallel, this algorithm runs in $O(lg(n))$ time.

\prob{6}
\begin{itemize}
  \item On a CREW PRAM with $n^2$ processors, we can think of assigning each processor to a unique element of the $n \times n$ matrix $C$ that results from the matrix multiplication of $A*B$.
    \begin{lstlisting}
each processor $C_{ij}$ does (simultaneously):
    read row $i$ from matrix $A$      // n ops
    read column $j$ from matrix $B$   // n ops
    write $C_{ij} = A_i * B_j$        // dot product of two vectors; n + n ops
    \end{lstlisting}
 Clearly, with all $n^2$ processors running at once (reading concurrently from wherever and writing exclusively to their individual answer slots), we get $O(n)$ time.
    \begin{itemize}
        \item Efficiency is $E(n,n^2) = \frac{n^3}{n^2*n} = 1$
        \item With $n^{\frac{1}{4}}$ processors we get that $T(n,n^{\frac{1}{4}}) \leq n^{\frac{7}{4}}*T(n,n^2) = n^{\frac{7}{4}}*n \rightarrow O(n^{\frac{11}{4}})$
    \end{itemize}

  \item On a CREW PRAM with $n^3$ processors, we recall first that for each element in $C = A*B$, we have that $C_{ij} = \sum_{k=1}^n A_{ik}B_{kj}$. Thus, each element of the result matrix is composed of $n$ terms, each of which is a product of two elements (one from each matrix). This means that matrix $C$ is actually composed of $n^2 \times n = n^3$ terms. Thus we have the following natural progression of the algorithm:
      \begin{lstlisting}
// Note that $n^3 \geq 2n^2$ for all $n \geq 2$

CR each element of both matrices with a separate processor    // constant time

// communication is instant
pass the necessary elements for the above specified pairwise multiplications

// there are $n^3$ of these, so this is constant
perform the pairwise multiplications

// lg(n) for each spot in C, in parallel (since we have n procs for each spot)
recursive addition alg given in class to sum the terms for each $C_{ij}$
      \end{lstlisting}

We can see that this algorithm runs in $1+0+1+ lg(n)$ time. In particular, note that we may calculate the terms for our $n^2$ sums in parallel, then use our previously specified (straightforward) addition algorithm to perform each n-length sum in $lg(n)$ time--also in parallel. So the largest time in our algorithm is $lg(n)$, giving $O(lg(n))$ time.
\begin{itemize}
  \item Efficiency is $E(n,n^3) = \frac{n^3}{n^3 * lg(n)} = \frac{1}{lg(n)}$.
  \item With $n^{\frac{1}{4}}$ processors we get that $T(n,n^{\frac{1}{4}}) \leq n^{\frac{11}{4}}*lg(n) \rightarrow O(n^{\frac{11}{4}}*lg(n))$
\end{itemize}

\item On a CREW PRAM with $\frac{n^3}{lg(n)}$ processors, we perform a similar algorithm to above, except that instead of concurrently performing the multiplications, we assign $\frac{n}{lg(n)}$ processors to each row of multiplications, which gives the same runtime as our typical format algorithm for commutative operations, that is $lg(n)$. Then we can perform our logarithmic sum algorithm for the elements in $C$, giving roughly $O(lg(n) + lg(n)) = O(lg(n))$ time.
\begin{itemize}
  \item Efficiency is $E(n,\frac{n^3}{lg(n)}) = \frac{n^3}{\frac{n^3}{lg(n)}*lg(n)} = 1$
  \item With $n^{\frac{1}{4}}$ processors we get that $T(n,n^{\frac{1}{4}}) \leq \frac{n^{\frac{11}{4}}}{lg(n)}*lg(n) \rightarrow O(n^{\frac{11}{4}})$
\end{itemize}

\item Efficiency and upper bound are the same as above...?
\end{itemize}


\end{document}
